{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cf53b1-964f-463d-8403-9512e7deb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cassava_df = pd.read_csv('/home/alumno/Desktop/datos/Computer Vision/cassava/cassava_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff21032d-894c-45a0-8f3e-0e6cd1ab7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ROOT_DIR = './cassava/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62decbf-564e-42ea-9604-07bc54250427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import lightning.pytorch as pl\n",
    "from torchvision import transforms \n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# --- 1. CLASE CASSAVADATASET ---\n",
    "class CassavaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para el conjunto de datos de Cassava.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image_id']\n",
    "        label = int(row['label']) \n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        # Cargamos la imagen y aseguramos el formato RGB\n",
    "        image = Image.open(img_path).convert('RGB') \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class CassavaDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_df: pd.DataFrame, image_dir: str, batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_df = data_df\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Now we transform to match the data the model was originally trained on\n",
    "        self.transform = transforms.Compose([\n",
    "             transforms.Resize(256),#, interpolation=InterpolationMode.BILINEAR),              \n",
    "             transforms.CenterCrop(224),          \n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # 1. Filtrar el DataFrame basado en la columna 'set'\n",
    "        train_df = self.data_df[self.data_df['set'] == 'train'].reset_index(drop=True)\n",
    "        val_df = self.data_df[self.data_df['set'] == 'val'].reset_index(drop=True)\n",
    "        test_df = self.data_df[self.data_df['set'] == 'test'].reset_index(drop=True)\n",
    "        \n",
    "        # 2. Creación de los objetos Dataset\n",
    "        self.train_ds = CassavaDataset(train_df, self.image_dir, transform=self.transform)\n",
    "        self.val_ds = CassavaDataset(val_df, self.image_dir, transform=self.transform)\n",
    "        self.test_ds = CassavaDataset(test_df, self.image_dir, transform=self.transform)\n",
    "\n",
    "        print(f\"Dataset sizes: Train={len(self.train_ds)}, Val={len(self.val_ds)}, Test={len(self.test_ds)}\")\n",
    "\n",
    "    # Métodos para crear los DataLoaders\n",
    "    def train_dataloader(self):\n",
    "        # Extract labels from the dataset dataframe\n",
    "        labels = self.train_ds.df['label'].values\n",
    "    \n",
    "        # Compute class frequencies\n",
    "        class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
    "        # Inverse of frequency as weights\n",
    "        weights = 1. / class_sample_count\n",
    "        # Assign weight to each sample\n",
    "        samples_weight = np.array([weights[t] for t in labels])\n",
    "    \n",
    "        # Define sampler\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=torch.DoubleTensor(samples_weight),\n",
    "            num_samples=len(samples_weight),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, num_workers = 2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, shuffle=False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9100b49-d2bc-4f48-a17d-6935656f1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3, wd=0., discriminative_lr=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.discriminative_lr = discriminative_lr # (lr, lr_mult)\n",
    "\n",
    "        self.training_step_outputs = defaultdict(float)\n",
    "        self.validation_step_outputs = defaultdict(float)\n",
    "\n",
    "    def get_layer_wise_lr(self, lr, lr_mult):\n",
    "\n",
    "        # Save layer names\n",
    "        layer_names = []\n",
    "        for idx, (name, param) in enumerate(self.model.named_parameters()):\n",
    "            layer_names.append(name)\n",
    "            print(f'{idx}: {name}')\n",
    "        \n",
    "        # Reverse layers\n",
    "        layer_names.reverse()\n",
    "        \n",
    "        # placeholder\n",
    "        parameters      = []\n",
    "        prev_group_name = layer_names[0].split('.')[0]\n",
    "        \n",
    "        # store params & learning rates\n",
    "        for idx, name in enumerate(layer_names):\n",
    "            \n",
    "            # parameter group name\n",
    "            cur_group_name = name.split('.')[0]\n",
    "            \n",
    "            # update learning rate\n",
    "            if cur_group_name != prev_group_name:\n",
    "                lr *= lr_mult\n",
    "            prev_group_name = cur_group_name\n",
    "            \n",
    "            # display info\n",
    "            print('Using discriminative learning rates')\n",
    "            print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "            \n",
    "            # append layer parameters\n",
    "            parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                            'lr':     lr}]\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        return torch.optim.Adam(\n",
    "            self.parameters() if self.discriminative_lr == None else self.get_layer_wise_lr(*self.discriminative_lr), \n",
    "            lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.training_step_outputs['loss'] += loss.detach().cpu()\n",
    "        self.training_step_outputs['steps'] += 1\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.validation_step_outputs['loss'] += loss.detach().cpu()\n",
    "        self.validation_step_outputs['steps'] += 1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self): \n",
    "        avg_loss = self.training_step_outputs['loss'] / self.training_step_outputs['steps']\n",
    "        print(f\"Average training loss for epoch {self.current_epoch}: {avg_loss.item():.4f}\")\n",
    "        self.training_step_outputs.clear() \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = self.validation_step_outputs['loss'] / self.validation_step_outputs['steps']\n",
    "        print(f\"Average validation loss for epoch {self.current_epoch}: {avg_loss.item():.4f}\")\n",
    "        self.validation_step_outputs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e265e3d9-bdf5-480b-9b86-2a761a64149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: Train=14977, Val=3210, Test=3210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightningModule(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "\n",
    "data_module = CassavaDataModule(cassava_df, IMAGE_ROOT_DIR)\n",
    "data_module.setup('fit')\n",
    "\n",
    "ckpt_path = \"/home/alumno/Desktop/datos/Computer Vision/lab 1/cassava_resnet18_tl_ft_disc_lr/version_0/checkpoints/best_valid_loss.ckpt\"\n",
    "\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "num_in_feat = model.fc.in_features\n",
    "model.fc = nn.Linear(num_in_feat, 5)\n",
    "\n",
    "# Load the best model. Note that we need to include all the LightningMNISTClassifier's __init__ parameters\n",
    "model = LightningModule.load_from_checkpoint(ckpt_path, model=model)\n",
    "model.cuda(0) # Move to GPU\n",
    "\n",
    "# Move to GPU if needed\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab2854b-b53c-45b6-9b9b-d4bd8179f5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_357319/918004032.py\", line 30, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n            ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/PIL/Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: './cassava/images/3481863039.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      6\u001b[39m y_trues, y_hats = [], []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_hat_logits\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1516\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1514\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1515\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1551\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/py313ml/.venv/lib/python3.13/site-packages/torch/_utils.py:769\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    767\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    768\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_357319/918004032.py\", line 30, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n            ~~~~~~~~~~^^^^^^^^^^\n  File \"/home/alumno/py313ml/.venv/lib/python3.13/site-packages/PIL/Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: './cassava/images/3481863039.jpg'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "y_trues, y_hats = [], []\n",
    "\n",
    "for (x, y_true) in iter(data_module.test_dataloader()):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat_logits = model(x.cuda())\n",
    "        y_hat = nn.Softmax(1)(y_hat_logits).argmax(1)\n",
    "        y_hat = y_hat.detach().cpu()\n",
    "\n",
    "    y_hats.append(y_hat)\n",
    "    y_trues.append(y_true)\n",
    "\n",
    "y_trues = torch.cat(y_trues)\n",
    "y_hats = torch.cat(y_hats)\n",
    "\n",
    "print(f'Acc.: {accuracy_score(y_trues, y_hats):.4f}')\n",
    "\n",
    "class_names = {\n",
    "    0: \"Cassava Bacterial Blight (CBB)\",\n",
    "    1: \"Cassava Brown Streak Disease (CBSD)\",\n",
    "    2: \"Cassava Green Mottle (CGM)\",\n",
    "    3: \"Cassava Mosaic Disease (CMD)\",\n",
    "    4: \"Healthy\"\n",
    "}\n",
    "class_labels = list(class_names.values())\n",
    "cm = confusion_matrix(y_trues, y_hats, labels=torch.arange(5))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.savefig(\"Confusion_matrix_4\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13bb2881-8625-45fc-bb77-8defb1f13b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Assuming y_trues and y_probs are already collected as in your previous code\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m n_classes = \u001b[38;5;28mlen\u001b[39m(\u001b[43mclass_labels\u001b[49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Binarize true labels\u001b[39;00m\n\u001b[32m     11\u001b[39m y_true_onehot = F.one_hot(y_trues, num_classes=n_classes).numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'class_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_trues and y_probs are already collected as in your previous code\n",
    "n_classes = len(class_labels)\n",
    "\n",
    "# Binarize true labels\n",
    "y_true_onehot = F.one_hot(y_trues, num_classes=n_classes).numpy()\n",
    "y_probs_np = y_probs.numpy()\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_onehot[:, i], y_probs_np[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"ROC_curves.png\", dpi=400)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (ML Master)",
   "language": "python",
   "name": "py313ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
